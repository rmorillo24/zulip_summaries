{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Testbed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weaviate\n",
    "\n",
    "from llama_index import StorageContext, SimpleDirectoryReader, ServiceContext, VectorStoreIndex, Document, get_response_synthesizer\n",
    "from llama_index.vector_stores import WeaviateVectorStore\n",
    "from llama_index.embeddings import LangchainEmbedding, OllamaEmbedding\n",
    "from llama_index.indices.document_summary import DocumentSummaryIndex\n",
    "from llama_index.llms import Ollama\n",
    "from llama_index.query_engine import RetrieverQueryEngine\n",
    "from llama_index.postprocessor import SimilarityPostprocessor\n",
    "from llama_index.retrievers import VectorIndexRetriever\n",
    "\n",
    "\n",
    "import box\n",
    "import yaml\n",
    "import warnings\n",
    "\n",
    "import logger as log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logger = log.init_logger(__name__)\n",
    "# logger.debug(\"start\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path='./.venv/config.yml'\n",
    "with open(config_path, 'r', encoding='utf8') as ymlfile:\n",
    "    cfg = box.Box(yaml.safe_load(ymlfile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = weaviate.Client(cfg.WEAVIATE_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Ollama(\n",
    "            model=cfg.LLM,\n",
    "            base_url=cfg.OLLAMA_BASE_URL,\n",
    "            temperature=cfg.TEMPERATURE\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OllamaEmbedding(model_name=cfg.LLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "service_context = ServiceContext.from_defaults(\n",
    "    embed_model=embeddings,\n",
    "    llm=llm\n",
    ")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"A1149\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = WeaviateVectorStore(\n",
    "    weaviate_client=client,\n",
    "    # index_name=cfg.INDEX_NAME\n",
    "    index_name = index_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_context = StorageContext.from_defaults(\n",
    "    vector_store=vector_store\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_synthesizer = get_response_synthesizer(\n",
    "    response_mode=\"tree_summarize\",\n",
    "    use_async=False,\n",
    "    service_context=service_context\n",
    ")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import VectorStoreIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_main = VectorStoreIndex.from_vector_store(\n",
    "    vector_store = vector_store,\n",
    "    service_context = service_context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = VectorIndexRetriever(\n",
    "    index=index_main,\n",
    "    similarity_top_k=4\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zulip_query_engine = index_main.as_query_engine()\n",
    "zulip_query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    "    node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=0.3)],\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = zulip_query_engine.query(\"What are the imlications of scheduling a brainstorm about partners\")\n",
    "# print(response)\n",
    "# for node in response.source_nodes:\n",
    "#     print(node.node_id, node.score, node.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### sample code to load a document into the current index (index_main)\n",
    "with open(\"data/_announcements/(Fibery) Brainstorm calls migration\", 'r') as file:\n",
    "    text = \" \".join(line.rstrip() for line in file)\n",
    "document = Document(text=text, doc_id=\"brainstorm\", metadata={\"stream\": \"_announcements\", \"doc_name\": \"brainstorm\"})\n",
    "documents=[document]\n",
    "# index_main.insert(document, show_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a custom prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.prompts import PromptTemplate\n",
    "poem_template = (\n",
    "    \"Context information is below.\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"Provide a poem with the informaton given.\"\n",
    "    \"answer the question: {query_str}\\n\"\n",
    "    \"Answer: \"\n",
    ")\n",
    "poem_prompt_template = PromptTemplate(poem_template)\n",
    "\n",
    "summary_template = (\n",
    "    \"context information is below.\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"There are different topics discussed in the information provided.\\n\"\n",
    "    \"For each topic create a markdown output with the following structure:\\n\"\n",
    "    \"## Topic:\\n\"\n",
    "    \"### Keypoints:\\n\"\n",
    "    \"### Decissions and actions:\\n\"\n",
    "    \"answer the question: {query_str}\\n\"\n",
    "    \"Answer: \"\n",
    ")\n",
    "summary_prompt_template = PromptTemplate(summary_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_synthesizer2 = get_response_synthesizer(\n",
    "    response_mode=\"tree_summarize\",\n",
    "    use_async=False,\n",
    "    service_context=service_context,\n",
    "    summary_template=summary_prompt_template\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "zulip_query_engine2 = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=response_synthesizer2,\n",
    "    node_postprocessors=[SimilarityPostprocessor(similarity_cutoff=0.3)],\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Prompt Key**: response_synthesizer:summary_template<br>**Text:** <br>\n"
     ]
    }
   ],
   "source": [
    "prompt_dict = zulip_query_engine2.get_prompts()\n",
    "for k,p in prompt_dict.items():\n",
    "    text_md = f\"**Prompt Key**: {k}<br>\" f\"**Text:** <br>\"\n",
    "    print(text_md)\n",
    "    # OUTPUT: **Prompt Key**: response_synthesizer:summary_template<br>**Text:** <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = RetrieverQueryEngine(retriever, response_synthesizer2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 106.45it/s]\n",
      "Summarizing documents:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: brainstorm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarizing documents: 100%|██████████| 1/1 [00:17<00:00, 17.79s/it]\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:01<00:00,  1.75s/it]\n"
     ]
    }
   ],
   "source": [
    "doc_summary_index2 = DocumentSummaryIndex.from_documents(\n",
    "    documents=documents,\n",
    "    storage_context=storage_context,\n",
    "    service_context=service_context,\n",
    "    response_synthesizer=response_synthesizer2,\n",
    "    show_progress=True,\n",
    "   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "## Topic: Organization of marketing and CS meetings\n",
      "\n",
      "### Keypoints:\n",
      "\n",
      "* There is a need to organize the marketing and CS meetings in a more efficient way.\n",
      "* The current setup has some meetings duplicating each other's efforts.\n",
      "* There are different aspects of marketing and CS that need to be addressed separately.\n",
      "\n",
      "### Decisions and actions:\n",
      "\n",
      "* Create a dedicated meeting for partnership projects/activities instead of discussing everything under a Marketing meeting.\n",
      "* Discuss the differences between marketing and CS meetings and whether they ought to be combined from an organization perspective or not.\n",
      "* Consider adding tags to tasks in Fibery, such as `aspect/marketing` and `product/balenaOS`, etc., to better filter and organize data.\n",
      "\n",
      "## Topic: Practical organization in Fibery\n",
      "\n",
      "### Keypoints:\n",
      "\n",
      "* There is a need for a more practical organization structure in Fibery.\n",
      "* The current setup may not be efficient and may require changes.\n",
      "* Implementing tags to tasks can help with filtering and organizing data.\n",
      "\n",
      "### Decisions and actions:\n",
      "\n",
      "* Discuss the need for a more practical organization structure in Fibery.\n",
      "* Explore different options for organizing data in Fibery, such as creating separate pages for marketing and CS meetings.\n",
      "* Implement tags to tasks to help with filtering and organizing data.\n",
      "\n",
      "## Topic: Brainstorm Calls and Patterns\n",
      "\n",
      "### Keypoints:\n",
      "\n",
      "* @**phil-d-wilson** and @**pipex** discussed the need for a more structured way to organize brainstorm calls and patterns in Fibery.\n",
      "* They identified that having a specific tag for each topic (e.g. `channels/partnerships`, `channels/device-support`) would be helpful for filtering and viewing related content in Fibery.\n",
      "* @**phil-d-wilson** added tags for `channels/partnerships` and `channels/device-support` to brainstorm calls and patterns.\n",
      "\n",
      "### Decisions and actions:\n",
      "\n",
      "* No decisions were made, but the discussion highlighted the need for a more structured organization of brainstorm calls and patterns in Fibery.\n",
      "* @**phil-d-wilson** and @**pipex** agreed to continue discussing this topic and exploring ways to improve the organization of brainstorm calls and patterns in Fibery.\n",
      "\n",
      "## Topic: Integration of Google Calendar events into Fibery\n",
      "\n",
      "### Keypoints:\n",
      "\n",
      "* The integration of Google Calendar events into Fibery is an option.\n",
      "* If the integration happens again, it's important to find a way to recreate the integration without losing the original event database.\n",
      "* The Brainstorm calls calendar only shows events on the admin calendar.\n",
      "\n",
      "### Decisions and actions:\n",
      "\n",
      "* None mentioned in the provided information.\n"
     ]
    }
   ],
   "source": [
    "response = doc_summary_index2.get_document_summary(\"brainstorm\")\n",
    "print(f'{response}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise SystemExit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary: using DocumentSummaryIndex and persisting in filesystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_summary_index = DocumentSummaryIndex.from_documents(\n",
    "    documents=documents,\n",
    "    storage_context=storage_context,\n",
    "    service_context=service_context,\n",
    "    response_synthesizer=response_synthesizer,\n",
    "    show_progress=True\n",
    "   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# persist summary in storage\n",
    "doc_summary_index.storage_context.persist(persist_dir=\"./storage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected to load a single index, but got 4 instead. Please specify index_id.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# retrieve summary from storage\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mindices\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloading\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_index_from_storage\n\u001b[0;32m----> 3\u001b[0m doc_summary_index2 \u001b[38;5;241m=\u001b[39m \u001b[43mload_index_from_storage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mservice_context\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Workspace/my-projects/zulip_summaries/.venv/lib/python3.10/site-packages/llama_index/indices/loading.py:40\u001b[0m, in \u001b[0;36mload_index_from_storage\u001b[0;34m(storage_context, index_id, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     37\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo index in storage context, check if you specified the right persist_dir.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     38\u001b[0m     )\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(indices) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 40\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     41\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected to load a single index, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(indices)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m instead. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     42\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease specify index_id.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     43\u001b[0m     )\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m indices[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mValueError\u001b[0m: Expected to load a single index, but got 4 instead. Please specify index_id."
     ]
    }
   ],
   "source": [
    "# retrieve summary from storage\n",
    "from llama_index.indices.loading import load_index_from_storage\n",
    "doc_summary_index2 = load_index_from_storage(storage_context=storage_context, service_context=service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Topic: Organizing Tasks with Tags\n",
      "\n",
      "### Keypoints:\n",
      "\n",
      "* Adding tags to tasks to indicate their relevance to specific aspects or products is a great idea.\n",
      "* Examples of tags that could be used include `aspect/marketing` for tasks related to marketing efforts, and `product/balenaOS` for tasks related to the BalenaOS product.\n",
      "* Using tags in this way can help us organize and filter our tasks more effectively.\n",
      "\n",
      "### Decisions and actions:\n",
      "\n",
      "* Implement the \"process/partnerships\" tag for Brainstorm calls related to partnerships.\n",
      "* Consider adding additional tags for other areas where a filter might be useful (e.g. security, device support).\n"
     ]
    }
   ],
   "source": [
    "print(doc_summary_index2.get_document_summary(\"brainstorm\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating a summary of an existing stored document using it's title\n",
    "I haven't been able to retreive the summary index. The index is not working. I've tried also persist in storage_context(storage_dir=\"./storage\"), but this is not storing or loading any indexes.\n",
    "Shame...\n",
    "\n",
    "from llama_index.indices.document_summary import DocumentSummaryIndexLLMRetriever\n",
    "from llama_index.indices.loading import load_index_from_storage, load_indices_from_storage\n",
    "doc_summary_index = load_indices_from_storage(storage_context=storage_context, index_id=index_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
